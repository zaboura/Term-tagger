{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abdelhak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PhraseMatcher.py\n",
    "# import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.matcher import PhraseMatcher #import PhraseMatcher class\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from progressbar import progressbar\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import minibatch, compounding\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import plac\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json \n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Language class with the English model 'en_core_web_sm' is loaded\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 7000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_spacy(data, n_iter = 10, load = False):\n",
    "    \n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    # load space model\n",
    "    if load:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        \n",
    "   \n",
    "        \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    # for _, annotations in TRAIN_DATA:\n",
    "    #      for ent in annotations.get('entities'):\n",
    "    ner.add_label('AstroTerm')\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "    # Resume training\n",
    "    #optimizer = nlp.resume_training()\n",
    "    #move_names = list(ner.move_names)\n",
    "\n",
    "    nlp.begin_training()\n",
    "\n",
    "    # Begin training by disabling other pipeline components\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # show warnings for misaligned entity spans once\n",
    "        optimizer = nlp.begin_training()\n",
    "\n",
    "        sizes = compounding(1.0, 5.0, 1.001)\n",
    "        # Training for n_iter iterations     \n",
    "        for itn in progressbar(range(n_iter)):\n",
    "            sleep(0.02)\n",
    "        # shuffle examples before training\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            # dictionary to store losses\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # Calling update() over the iteration\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.25, losses=losses)\n",
    "                #print(\"Losses\", losses)\n",
    "    return nlp\n",
    "\n",
    "def extract_entities(doc):\n",
    "    dict_new_ents = {}\n",
    "    list_new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check\n",
    "        if ent.label_ == \"AstroTerm\":\n",
    "            \n",
    "            \n",
    "            list_new_ents.append((ent.start_char, ent.end_char, ent.label_))      \n",
    "    \n",
    "    dict_new_ents['entities'] = list_new_ents\n",
    "            \n",
    "    return (doc.text,dict_new_ents )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the list of terminology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_corpus = pd.read_excel('astronomy.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list containing the pharses to be matched\n",
    "terminology_list = []\n",
    "for term in terms_corpus['key']:\n",
    "    terminology_list.append(term[term.find(':')+2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# read_files = glob.glob(\"corpus/Astromony_*.txt\")\n",
    "# with open(\"corpus/result.txt\", \"wb\") as outfile:\n",
    "#     for f in read_files:\n",
    "#         with open(f, \"rb\") as infile:\n",
    "#             outfile.write(infile.read())\n",
    "\n",
    "# the input text string is converted to a Document object\n",
    "\n",
    "file = open('corpus/result.txt')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rule_based = English()\n",
    "ruler = EntityRuler(nlp_rule_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create patterns\n",
    "patterns = []\n",
    "\n",
    "for term in terminology_list:\n",
    "    dct = {}\n",
    "    temp = term.split()\n",
    "    if len(temp) == 1:\n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = temp[0]\n",
    "        patterns.append(dct)\n",
    "    else:\n",
    "        lst = []\n",
    "        for item in temp:\n",
    "            dct_temp = {}\n",
    "            dct_temp[\"lower\"] = item\n",
    "            \n",
    "            lst.append(dct_temp)\n",
    "            \n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = lst\n",
    "        patterns.append(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)\n",
    "nlp_rule_based.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for doc in nltk.tokenize.sent_tokenize(text):\n",
    "    doc = nlp_rule_based(doc)\n",
    "    train_data.append(extract_entities(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdelhak/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n",
      "100% (100 of 100) |#####################| Elapsed Time: 21:09:17 Time: 21:09:17\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "#nlp = train_spacy(train_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to output_dir\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "output_dir = \"output_dir\"\n",
    "# if output_dir is not None:\n",
    "#     output_dir = Path(output_dir)\n",
    "#     if not output_dir.exists():\n",
    "#         output_dir.mkdir()\n",
    "#     nlp.to_disk(output_dir)\n",
    "#     print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your testing text: Home/News/Where is the edge of the universe? 4 Where is the edge of the universe? We may never know. By Eric Betz  |  Published: Tuesday, November 10, 2020 RELATED TOPICS: ASTROPHYSICS sphere Jay Smith When Galileo Galilei pointed his first telescope to the heavens in 1610, he discovered “congeries of innumerable stars” hidden in the band of light called the Milky Way. Our cosmos grew exponentially that day. Roughly three centuries later, the cosmic bounds exploded once again when astronomers built telescopes big enough to show the Milky Way is just one of many “island universes.” Soon they learned the universe was expanding, too, with galaxies retreating from each other at ever-accelerating speeds.  Since then, ever-larger telescopes have shown the observable universe spans an incomprehensible 92 billion light-years across and contains perhaps 2 trillion galaxies. And yet, astronomers are still left wondering how much more universe is out there, beyond what they observe.  “The universe has always been slightly larger than what we can see,” says Virginia Trimble of the University of California, Irvine, an astronomer and expert in the field’s history.  Building bigger telescopes won’t help extend the cosmos anymore. “Telescopes only observe the observable. You can’t see back in time further than the age of the universe,” explains Nobel Prize-winning cosmologist John Mather of NASA’s Goddard Space Flight Center, who’s also chief scientist for the James Webb Space Telescope. “So we are totally limited. We’ve already seen as far as you could possibly imagine.” At the edge, we see the leftover glow from the Big Bang — the so-called cosmic microwave background radiation (CMB). But this isn’t some magical edge of the universe. Our cosmos keeps going. We just may never know how far. In recent decades, cosmologists have tried to solve this mystery by first determining the universe’s shape, like the ancient Greek mathematician Eratosthenes calculating Earth’s size using simple trigonometry. In theory, our universe can have one of three possible shapes, each one dependent on the curvature of space itself: saddle shaped (negative curvature), spherical (positive curvature) or flat (no curvature).  Few have championed a saddle-shaped universe, but a spherical cosmos makes sense to us earthlings. Earth is round, as are the sun and planets. A spherical universe would let you sail into the cosmos in any direction and end up back where you started, like Ferdinand Magellan’s crew circumnavigating the globe. Einstein called this model a “finite yet unbounded universe.”  But starting in the late 1980s, a series of orbiting observatories built to study the CMB made increasingly precise measurements showing that space has no curvature at all. It’s flat to the limits of what astronomers can measure — if it is a sphere, it’s a sphere so huge that even our entire observable universe doesn’t register any curvature.  “The universe is flat like an [endless] sheet of paper,” says Mather. “According to this, you could continue infinitely far in any direction and the universe would be just the same, more or less.” You’d never come to an edge of this flat universe; you’d only find more and more galaxies.  That’s all well and good with most astronomers. A flat universe agrees with both observation and theory, so the idea now sits at the heart of modern cosmology.  The problem is that, unlike a spherical universe, a flat one can be infinite — or not. And there’s no real way to tell the difference. “What could you look for to see whether there’s an infinite universe?” Trimble says. “Nobody quite knows.”  So instead, astronomers hope an answer can come from theory — a model that could offer indirect proof one way or the other. For example, the Standard Model of physics predicted the existence of numerous particles, like the Higgs Boson, years before they were actually discovered. Yet physicists still presumed those particles were real.  “If you have a good description of everything you’ve observed so far and it predicts something is true, then you expect it is,” Trimble says. “That’s how most scientists think about how science works.”\n"
     ]
    }
   ],
   "source": [
    "# text test\n",
    "text_test = input(\"Enter your testing text: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test text\n",
    "#file = open('test_corpus/text09.txt')\n",
    "#text_test = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('universe', 'AstroTerm'): 20,\n",
       "         ('telescope', 'AstroTerm'): 1,\n",
       "         ('light', 'AstroTerm'): 2,\n",
       "         ('cosmos', 'AstroTerm'): 5,\n",
       "         ('astronomer', 'AstroTerm'): 1,\n",
       "         ('expert', 'AstroTerm'): 1,\n",
       "         ('cosmologist', 'AstroTerm'): 1,\n",
       "         ('Space Telescope', 'AstroTerm'): 1,\n",
       "         ('totally', 'AstroTerm'): 1,\n",
       "         ('glow', 'AstroTerm'): 1,\n",
       "         ('Big Bang', 'AstroTerm'): 1,\n",
       "         ('so-called', 'AstroTerm'): 1,\n",
       "         ('space', 'AstroTerm'): 2,\n",
       "         ('observation', 'AstroTerm'): 1,\n",
       "         ('cosmology', 'AstroTerm'): 1})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the neural network model\n",
    "doc = nlp(text_test)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#print(\"Entities\", entities)\n",
    "counter = Counter(entities)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('universe', 'AstroTerm'): 20,\n",
       "         ('telescope', 'AstroTerm'): 1,\n",
       "         ('light', 'AstroTerm'): 2,\n",
       "         ('cosmos', 'AstroTerm'): 5,\n",
       "         ('astronomer', 'AstroTerm'): 1,\n",
       "         ('expert', 'AstroTerm'): 1,\n",
       "         ('Space Telescope', 'AstroTerm'): 1,\n",
       "         ('totally', 'AstroTerm'): 1,\n",
       "         ('glow', 'AstroTerm'): 1,\n",
       "         ('Big Bang', 'AstroTerm'): 1,\n",
       "         ('so-called', 'AstroTerm'): 1,\n",
       "         ('space', 'AstroTerm'): 2,\n",
       "         ('observation', 'AstroTerm'): 1,\n",
       "         ('cosmology', 'AstroTerm'): 1})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run rule based model\n",
    "doc = nlp_rule_based(text_test)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#print(\"Entities\", entities)\n",
    "counter = Counter(entities)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
