{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhraseMatcher.py\n",
    "# import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.matcher import PhraseMatcher #import PhraseMatcher class\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "from time import sleep\n",
    "from progressbar import progressbar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# Language class with the English model 'en_core_web_sm' is loaded\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 7000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_corpus = pd.read_excel('astronomy.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list containing the pharses to be matched\n",
    "terminology_list = []\n",
    "for term in terms_corpus['key']:\n",
    "    terminology_list.append(term[term.find(':')+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "read_files = glob.glob(\"corpus/Astromony_2*.txt\")\n",
    "\n",
    "with open(\"corpus/result.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "\n",
    "\n",
    "# the input text string is converted to a Document object\n",
    "\n",
    "\n",
    "\n",
    "file  = open('corpus/result.txt')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"AstroTerm\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "# create the PhraseMatcher object\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "#print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOB_tagger = [(X, X.ent_iob_, X.ent_type_) for X in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the phrases into document object using nlp.make_doc to #speed up.\n",
    "patterns = [nlp.make_doc(text) for text in terminology_list]\n",
    "# add the patterns to the matcher object without any callbacks\n",
    "matcher.add(\"TerminologyList\", collect_sents, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the matcher object the document object and it will return #match_id, start and stop indexes of the matched words\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess training data\n",
    "TRAIN_DATA = []\n",
    "LABEL = 'AstroTerm'\n",
    "for dic in matched_sents:\n",
    "    inner_list = []\n",
    "    dict_item = {}\n",
    "    inner_list.append((dic['ents'][0]['start'],dic['ents'][0]['end'],LABEL))\n",
    "    dict_item['entities'] = inner_list\n",
    "    TRAIN_DATA.append((dic['text'],dict_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "\n",
    "#nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "\n",
    "\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "        \n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 1:00:59 Time:  1:00:59\n"
     ]
    }
   ],
   "source": [
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "n_iter = 100\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # Training for n_iter iterations     \n",
    "    for itn in progressbar(range(n_iter)):\n",
    "        sleep(0.02)\n",
    "    # shuffle examples before training\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "        # ictionary to store losses\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            # Calling update() over the iteration\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            #print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to output_dir\n"
     ]
    }
   ],
   "source": [
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = [\"The Astronomy is hard science\", \"Telescope is the main tool in astronomy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from output_dir\n",
      "Entities [('Astronomy', 'AstroTerm')]\n",
      "Tokens [('The', '', 2), ('Astronomy', 'AstroTerm', 3), ('is', '', 2), ('hard', '', 2), ('science', '', 2)]\n",
      "Entities [('astronomy', 'AstroTerm')]\n",
      "Tokens [('Telescope', '', 2), ('is', '', 2), ('the', '', 2), ('main', '', 2), ('tool', '', 2), ('in', '', 2), ('astronomy', 'AstroTerm', 3)]\n"
     ]
    }
   ],
   "source": [
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "for text in TEST:\n",
    "    doc = nlp2(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
