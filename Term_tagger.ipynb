{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abdelhak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PhraseMatcher.py\n",
    "# import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.matcher import PhraseMatcher #import PhraseMatcher class\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from progressbar import progressbar\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import minibatch, compounding\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import plac\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json \n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Language class with the English model 'en_core_web_sm' is loaded\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 7000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the list of terminology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_corpus = pd.read_excel('astronomy.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list containing the pharses to be matched\n",
    "terminology_list = []\n",
    "for term in terms_corpus['key']:\n",
    "    terminology_list.append(term[term.find(':')+2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "read_files = glob.glob(\"corpus/Astromony_*.txt\")\n",
    "with open(\"corpus/result.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "# the input text string is converted to a Document object\n",
    "\n",
    "file = open('corpus/result.txt')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rule_based = English()\n",
    "ruler = EntityRuler(nlp_rule_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create patterns\n",
    "patterns = []\n",
    "\n",
    "for term in terminology_list:\n",
    "    dct = {}\n",
    "    temp = term.split()\n",
    "    if len(temp) == 1:\n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = temp[0]\n",
    "        patterns.append(dct)\n",
    "    else:\n",
    "        lst = []\n",
    "        for item in temp:\n",
    "            dct_temp = {}\n",
    "            dct_temp[\"lower\"] = item\n",
    "            \n",
    "            lst.append(dct_temp)\n",
    "            \n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = lst\n",
    "        patterns.append(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)\n",
    "nlp_rule_based.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc):\n",
    "    dict_new_ents = {}\n",
    "    list_new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check\n",
    "        if ent.label_ == \"AstroTerm\":\n",
    "            \n",
    "            \n",
    "            list_new_ents.append((ent.start_char, ent.end_char, ent.label_))      \n",
    "    \n",
    "    dict_new_ents['entities'] = list_new_ents\n",
    "            \n",
    "    return (doc.text,dict_new_ents )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for doc in nltk.tokenize.sent_tokenize(text):\n",
    "    doc = nlp_rule_based(doc)\n",
    "    train_data.append(extract_entities(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_spacy(data, n_iter = 10, load = False):\n",
    "    \n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    # load space model\n",
    "    if load:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        \n",
    "   \n",
    "        \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    # for _, annotations in TRAIN_DATA:\n",
    "    #      for ent in annotations.get('entities'):\n",
    "    ner.add_label('AstroTerm')\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "    # Resume training\n",
    "    #optimizer = nlp.resume_training()\n",
    "    #move_names = list(ner.move_names)\n",
    "\n",
    "    nlp.begin_training()\n",
    "\n",
    "    # Begin training by disabling other pipeline components\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # show warnings for misaligned entity spans once\n",
    "        optimizer = nlp.begin_training()\n",
    "\n",
    "        sizes = compounding(1.0, 5.0, 1.001)\n",
    "        # Training for n_iter iterations     \n",
    "        for itn in progressbar(range(n_iter)):\n",
    "            sleep(0.02)\n",
    "        # shuffle examples before training\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            # dictionary to store losses\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # Calling update() over the iteration\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.25, losses=losses)\n",
    "                #print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdelhak/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n",
      "100% (100 of 100) |#####################| Elapsed Time: 21:09:17 Time: 21:09:17\n"
     ]
    }
   ],
   "source": [
    "nlp = train_spacy(train_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to output_dir\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output_dir\"\n",
    "# if output_dir is not None:\n",
    "#     output_dir = Path(output_dir)\n",
    "#     if not output_dir.exists():\n",
    "#         output_dir.mkdir()\n",
    "#     nlp.to_disk(output_dir)\n",
    "#     print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test_corpus/text04.txt')\n",
    "text_test = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('spectroscopic', 'AstroTerm'): 2,\n",
       "         ('creativity', 'AstroTerm'): 1,\n",
       "         ('happy', 'AstroTerm'): 1,\n",
       "         ('expert', 'AstroTerm'): 1,\n",
       "         ('cosmos', 'AstroTerm'): 1,\n",
       "         ('galaxy', 'AstroTerm'): 2,\n",
       "         ('interstellar gas', 'AstroTerm'): 1,\n",
       "         ('dust', 'AstroTerm'): 1,\n",
       "         ('star', 'AstroTerm'): 1,\n",
       "         ('universe', 'AstroTerm'): 2,\n",
       "         ('Black Hole', 'AstroTerm'): 1,\n",
       "         ('black hole', 'AstroTerm'): 1,\n",
       "         ('planet', 'AstroTerm'): 1,\n",
       "         ('astronomy', 'AstroTerm'): 1,\n",
       "         ('light', 'AstroTerm'): 2,\n",
       "         ('collaboration', 'AstroTerm'): 1,\n",
       "         ('telescope', 'AstroTerm'): 2,\n",
       "         ('astrophysics', 'AstroTerm'): 1,\n",
       "         ('Southern Hemisphere', 'AstroTerm'): 1})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text_test)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#print(\"Entities\", entities)\n",
    "counter = Counter(entities)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('spectroscopic', 'AstroTerm'): 2,\n",
       "         ('happy', 'AstroTerm'): 1,\n",
       "         ('expert', 'AstroTerm'): 1,\n",
       "         ('cosmos', 'AstroTerm'): 1,\n",
       "         ('galaxy', 'AstroTerm'): 2,\n",
       "         ('interstellar gas', 'AstroTerm'): 1,\n",
       "         ('dust', 'AstroTerm'): 1,\n",
       "         ('star', 'AstroTerm'): 1,\n",
       "         ('universe', 'AstroTerm'): 2,\n",
       "         ('Black Hole', 'AstroTerm'): 1,\n",
       "         ('black hole', 'AstroTerm'): 1,\n",
       "         ('planet', 'AstroTerm'): 1,\n",
       "         ('astronomy', 'AstroTerm'): 1,\n",
       "         ('light', 'AstroTerm'): 2,\n",
       "         ('collaboration', 'AstroTerm'): 1,\n",
       "         ('telescope', 'AstroTerm'): 2,\n",
       "         ('astrophysics', 'AstroTerm'): 1,\n",
       "         ('Southern Hemisphere', 'AstroTerm'): 1})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp_rule_based(text_test)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#print(\"Entities\", entities)\n",
    "counter = Counter(entities)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = input(\"Enter your testing text: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
