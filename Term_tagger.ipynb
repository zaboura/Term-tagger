{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abdelhak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PhraseMatcher.py\n",
    "# import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.matcher import PhraseMatcher #import PhraseMatcher class\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from progressbar import progressbar\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import minibatch, compounding\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import plac\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json \n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Language class with the English model 'en_core_web_sm' is loaded\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 7000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_spacy(data, n_iter = 10, load = False):\n",
    "    \n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    # load space model\n",
    "    if load:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        \n",
    "   \n",
    "        \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    # for _, annotations in TRAIN_DATA:\n",
    "    #      for ent in annotations.get('entities'):\n",
    "    ner.add_label('AstroTerm')\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "    # Resume training\n",
    "    #optimizer = nlp.resume_training()\n",
    "    #move_names = list(ner.move_names)\n",
    "\n",
    "    nlp.begin_training()\n",
    "\n",
    "    # Begin training by disabling other pipeline components\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # show warnings for misaligned entity spans once\n",
    "        optimizer = nlp.begin_training()\n",
    "\n",
    "        sizes = compounding(1.0, 5.0, 1.001)\n",
    "        # Training for n_iter iterations     \n",
    "        for itn in progressbar(range(n_iter)):\n",
    "            sleep(0.02)\n",
    "        # shuffle examples before training\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            # dictionary to store losses\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # Calling update() over the iteration\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.25, losses=losses)\n",
    "                #print(\"Losses\", losses)\n",
    "    return nlp\n",
    "\n",
    "def extract_entities(doc):\n",
    "    dict_new_ents = {}\n",
    "    list_new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check\n",
    "        if ent.label_ == \"AstroTerm\":\n",
    "            \n",
    "            \n",
    "            list_new_ents.append((ent.start_char, ent.end_char, ent.label_))      \n",
    "    \n",
    "    dict_new_ents['entities'] = list_new_ents\n",
    "            \n",
    "    return (doc.text,dict_new_ents )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the list of terminology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_corpus = pd.read_excel('astronomy.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list containing the pharses to be matched\n",
    "terminology_list = []\n",
    "for term in terms_corpus['key']:\n",
    "    terminology_list.append(term[term.find(':')+2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# read_files = glob.glob(\"corpus/Astromony_*.txt\")\n",
    "# with open(\"corpus/result.txt\", \"wb\") as outfile:\n",
    "#     for f in read_files:\n",
    "#         with open(f, \"rb\") as infile:\n",
    "#             outfile.write(infile.read())\n",
    "\n",
    "# the input text string is converted to a Document object\n",
    "\n",
    "file = open('corpus/result.txt')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rule_based = English()\n",
    "ruler = EntityRuler(nlp_rule_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create patterns\n",
    "patterns = []\n",
    "\n",
    "for term in terminology_list:\n",
    "    dct = {}\n",
    "    temp = term.split()\n",
    "    if len(temp) == 1:\n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = temp[0]\n",
    "        patterns.append(dct)\n",
    "    else:\n",
    "        lst = []\n",
    "        for item in temp:\n",
    "            dct_temp = {}\n",
    "            dct_temp[\"lower\"] = item\n",
    "            \n",
    "            lst.append(dct_temp)\n",
    "            \n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = lst\n",
    "        patterns.append(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)\n",
    "nlp_rule_based.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for doc in nltk.tokenize.sent_tokenize(text):\n",
    "    doc = nlp_rule_based(doc)\n",
    "    train_data.append(extract_entities(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdelhak/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n",
      "100% (100 of 100) |#####################| Elapsed Time: 21:09:17 Time: 21:09:17\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "#nlp = train_spacy(train_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to output_dir\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "output_dir = \"output_dir\"\n",
    "# if output_dir is not None:\n",
    "#     output_dir = Path(output_dir)\n",
    "#     if not output_dir.exists():\n",
    "#         output_dir.mkdir()\n",
    "#     nlp.to_disk(output_dir)\n",
    "#     print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your testing text: In this rendition of the human timeline, we don’t abandon heavy industry. We learn to manufacture what we need to maintain our lives in the cold vacuum of space, just in time to give Earth a break.  The race to build an industrial foundation in space has already begun, too: Musk promises Mars Base Alpha by 2028; Bezos’ own Blue Origin is working on a “sustained human presence on the Moon;” and NASA’s Lunar Gateway, a permanent orbital station, is set to go into operation by the end of the decade.  In 40 years, launch costs have fallen from $85,000 per kilogram to less than $1,000/kg, and NASA hopes to get this under $100/kg in the next few years. This trajectory makes space-mining advocate and Skycorp CEO Dennis Wingo more certain than ever that we are on the cusp of a new era of space mining. He reiterates to Astronomy that “industrial activity on the Moon is how we can make things better here on Earth.”  Instead of returning raw materials from the Moon to Earth, which Wingo suggests would “be kind of like shipping dirt from Jakarta to the U.S.,” the space-mining industry would chase profits by finding ways to process raw materials directly at their icy, remote sources. On the horizon, he envisions a solar-powered lunar base capable of producing the gigawatt-level power needed for mining\n"
     ]
    }
   ],
   "source": [
    "# text test\n",
    "text_test = input(\"Enter your testing text: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test text\n",
    "#file = open('test_corpus/text09.txt')\n",
    "#text_test = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('astronomical object', 'AstroTerm'): 1,\n",
       "         ('imaginary line', 'AstroTerm'): 1,\n",
       "         ('gravitationally', 'AstroTerm'): 1,\n",
       "         ('planet', 'AstroTerm'): 2,\n",
       "         ('perpendicular', 'AstroTerm'): 1,\n",
       "         ('celestial equator', 'AstroTerm'): 1,\n",
       "         ('equinox', 'AstroTerm'): 2,\n",
       "         ('apparent', 'AstroTerm'): 1,\n",
       "         ('geocentric', 'AstroTerm'): 1,\n",
       "         ('solstice', 'AstroTerm'): 1,\n",
       "         ('velocity', 'AstroTerm'): 3,\n",
       "         ('kinetic energy', 'AstroTerm'): 1,\n",
       "         ('mass', 'AstroTerm'): 4,\n",
       "         ('orbit', 'AstroTerm'): 1,\n",
       "         ('radius', 'AstroTerm'): 1,\n",
       "         ('star', 'AstroTerm'): 2})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the neural network model\n",
    "doc = nlp(text_test)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#print(\"Entities\", entities)\n",
    "counter = Counter(entities)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('astronomical object', 'AstroTerm'): 1,\n",
       "         ('imaginary line', 'AstroTerm'): 1,\n",
       "         ('gravitationally', 'AstroTerm'): 1,\n",
       "         ('planet', 'AstroTerm'): 2,\n",
       "         ('perpendicular', 'AstroTerm'): 1,\n",
       "         ('celestial equator', 'AstroTerm'): 1,\n",
       "         ('equinox', 'AstroTerm'): 2,\n",
       "         ('apparent', 'AstroTerm'): 1,\n",
       "         ('geocentric', 'AstroTerm'): 1,\n",
       "         ('solstice', 'AstroTerm'): 1,\n",
       "         ('velocity', 'AstroTerm'): 3,\n",
       "         ('kinetic energy', 'AstroTerm'): 1,\n",
       "         ('mass', 'AstroTerm'): 4,\n",
       "         ('orbit', 'AstroTerm'): 1,\n",
       "         ('radius', 'AstroTerm'): 1,\n",
       "         ('star', 'AstroTerm'): 2})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run rule based model\n",
    "doc = nlp_rule_based(text_test)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#print(\"Entities\", entities)\n",
    "counter = Counter(entities)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
