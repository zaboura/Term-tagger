{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhraseMatcher.py\n",
    "# import necessary modules\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.matcher import PhraseMatcher #import PhraseMatcher class\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "from time import sleep\n",
    "from progressbar import progressbar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json \n",
    "\n",
    "# Language class with the English model 'en_core_web_sm' is loaded\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 7000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_corpus = pd.read_excel('astronomy.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list containing the pharses to be matched\n",
    "terminology_list = []\n",
    "for term in terms_corpus['key']:\n",
    "    terminology_list.append(term[term.find(':')+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# read_files = glob.glob(\"corpus/Astromony_2*.txt\")\n",
    "\n",
    "# with open(\"corpus/result.txt\", \"wb\") as outfile:\n",
    "#     for f in read_files:\n",
    "#         with open(f, \"rb\") as infile:\n",
    "#             outfile.write(infile.read())\n",
    "\n",
    "\n",
    "\n",
    "# the input text string is converted to a Document object\n",
    "\n",
    "\n",
    "\n",
    "file  = open('corpus/result.txt')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"AstroTerm\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "# create the PhraseMatcher object\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "#print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the phrases into document object using nlp.make_doc to #speed up.\n",
    "#patterns = [nlp.make_doc(text) for text in terminology_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create patterns\n",
    "patterns = []\n",
    "\n",
    "for term in terminology_list:\n",
    "    dct = {}\n",
    "    temp = term.split()\n",
    "    if len(temp) == 1:\n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = temp[0]\n",
    "        patterns.append(dct)\n",
    "    else:\n",
    "        lst = []\n",
    "        for item in temp:\n",
    "            dct_temp = {}\n",
    "            dct_temp[\"lower\"] = item\n",
    "            \n",
    "            lst.append(dct_temp)\n",
    "            \n",
    "        dct[\"label\"] = \"AstroTerm\"\n",
    "        dct[\"pattern\"] = lst\n",
    "        patterns.append(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EntityRuler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-550b8fe47af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mruler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntityRuler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mruler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EntityRuler' is not defined"
     ]
    }
   ],
   "source": [
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the patterns to the matcher object without any callbacks\n",
    "#matcher.add(\"TerminologyList\", collect_sents, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the matcher object the document object and it will return #match_id, start and stop indexes of the matched words\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess training data\n",
    "TRAIN_DATA = []\n",
    "LABEL = 'AstroTerm'\n",
    "for dic in matched_sents:\n",
    "    inner_list = []\n",
    "    dict_item = {}\n",
    "    inner_list.append((dic['ents'][0]['start'],dic['ents'][0]['end'],LABEL))\n",
    "    dict_item['entities'] = inner_list\n",
    "    TRAIN_DATA.append((dic['text'],dict_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "\n",
    "# #nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "\n",
    "# # Add the new label to ner\n",
    "# ner.add_label(LABEL)\n",
    "\n",
    "# # get names of other pipes to disable them during training\n",
    "# other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "# for _, annotations in TRAIN_DATA:\n",
    "#     for ent in annotations.get('entities'):\n",
    "#         ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_spacy(data, n_iter = 10, load = True):\n",
    "    \n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    # load space model\n",
    "    if load:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        \n",
    "   \n",
    "        \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    # for _, annotations in TRAIN_DATA:\n",
    "    #      for ent in annotations.get('entities'):\n",
    "    ner.add_label('AstroTerm')\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "    # Resume training\n",
    "    #optimizer = nlp.resume_training()\n",
    "    #move_names = list(ner.move_names)\n",
    "\n",
    "    nlp.begin_training()\n",
    "\n",
    "    # Begin training by disabling other pipeline components\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # show warnings for misaligned entity spans once\n",
    "        optimizer = nlp.begin_training()\n",
    "\n",
    "        sizes = compounding(1.0, 5.0, 1.001)\n",
    "        # Training for n_iter iterations     \n",
    "        for itn in progressbar(range(n_iter)):\n",
    "            sleep(0.02)\n",
    "        # shuffle examples before training\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            # dictionary to store losses\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # Calling update() over the iteration\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.25, losses=losses)\n",
    "                #print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp2 = train_spacy(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if output_dir is not None:\n",
    "#     output_dir = Path(output_dir)\n",
    "#     if not output_dir.exists():\n",
    "#         output_dir.mkdir()\n",
    "#     nlp.to_disk(output_dir)\n",
    "#     print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "file  = open('test_corpus/text01.txt')\n",
    "Test = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the saved model\n",
    "# print(\"Loading from\", output_dir)\n",
    "# nlp2 = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp2('Physicists began using supercomputers to obtain solutions to this famously hard problem back in the 1960s. In 2000, with no solutions in sight, Kip Thorne, 2018 Nobel Laureate and one of the designers of LIGO, famously bet that there would be an observation of gravitational waves before a numerical solution was reached.')\n",
    "# print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "# print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open('train_data.json',) \n",
    "data = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "\n",
    "for txt, dct in data[:200]:\n",
    "    inner_list = []\n",
    "    dict_item = {}\n",
    "    if len(dct['entities']) > 1:\n",
    "        for i in range(len(dct['entities'])-1):\n",
    "\n",
    "            beg_prev_ent = int(dct['entities'][i][0])\n",
    "            end_prev_ent = int(dct['entities'][i][1])\n",
    "            beg_next_ent = int(dct['entities'][i+1][0])\n",
    "            end_next_ent = int(dct['entities'][i+1][1])\n",
    "\n",
    "            if beg_next_ent - end_prev_ent == 1:  #the closet terms\n",
    "                inner_list.append((beg_prev_ent,end_next_ent, 'AstroTerm'))\n",
    "                dict_item['entities'] = inner_list\n",
    "                data_train.append((txt,dict_item))\n",
    "            \n",
    "                \n",
    "            if beg_next_ent - end_prev_ent < 0:\n",
    "                inner_list.append((min(beg_prev_ent,beg_next_ent), max(end_prev_ent,end_next_ent), 'AstroTerm'))\n",
    "                \n",
    "                #print('case.   <0 :', inner_list)\n",
    "                \n",
    "            else:\n",
    "          \n",
    "                inner_list.append((beg_prev_ent, end_prev_ent, 'AstroTerm'))\n",
    "                #inner_list.append((beg_next_ent, end_next_ent, 'AstroTerm'))\n",
    "\n",
    "        # unwanted element of the list\n",
    "        idx_list = []\n",
    "        for i in range(1,len(inner_list)):\n",
    "\n",
    "            if inner_list[i][0] in [inner_list[i-1][0], inner_list[i-1][1]] or inner_list[i][1] in [inner_list[i-1][0],  inner_list[i-1][1]]:\n",
    "                idx_list.append(i)\n",
    "                #print(\"beg : \", inner_list[i][0])\n",
    "                #print(\"end : \", inner_list[i][1])\n",
    "        np.delete(inner_list,idx_list).tolist()\n",
    "\n",
    "        dict_item['entities'] = inner_list\n",
    "\n",
    "#         print(dct['entities'])\n",
    "#         print('\\n')\n",
    "#         print(inner_list)\n",
    "#         print('\\n\\n\\n')\n",
    "\n",
    "    else:\n",
    "        dict_item['entities'] = [(dct['entities'][0][0],dct['entities'][0][1], 'AstroTerm')]\n",
    "\n",
    "    data_train.append((txt, dict_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_test = train_spacy(data_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = [('solar system .', {'entities': [(0, 12, 'AstroTerm')]}),\n",
    " (\"If Orion hasn't risen yet, you can look high in the east for the little cluster of blue stars called the Pleiades.\",\n",
    "  {'entities': [(3, 8, 'AstroTerm'),(105, 113, 'AstroTerm')]}),\n",
    " ('(Above:\\xa0Taurus contains interesting targets for naked eye, binoculars, and large and small telescopes.',\n",
    "  {'entities': [(8, 14, 'AstroTerm'),(91, 101, 'AstroTerm')]}),\n",
    " ('★ Kuiper Belt', {'entities': [(2, 13, 'AstroTerm')]}),             \n",
    " (\"An apparent gap between Saturn's A and B rings.\",\n",
    "  {'entities': [(24, 30, 'AstroTerm')]}), \n",
    " ('A description of gravity formulated by Albert Einstein, which explains that gravity affects the geometry of space and the flow of time.',\n",
    "  {'entities': [(39, 54, 'AstroTerm'),(76, 84, 'AstroTerm')]}),\n",
    " ('A common type of reflector telescope designed by Sir Isaac Newton.\\n',\n",
    "  {'entities': [(27, 36, 'AstroTerm'), (53, 65, 'AstroTerm')]}),\n",
    " ('A telescope whose design incorporates innovative features, such as adaptive optics.',\n",
    "  {'entities': [(2, 11, 'AstroTerm')]}),\n",
    " ('A complete circle (360º)      has 2o radians (6.283r), so one radian is about 57.296º.\\nRADIANT\\n',\n",
    "  {'entities': [(62, 68, 'AstroTerm')]}),\n",
    " (\"If the Moon orbited in the     same plane as the Earth around the Sun, the Sun would be eclipsed every month,     but the Moon's orbit is in a slightly inclined plane, making eclipses of the     Sun rare.\\n\",\n",
    "  {'entities': [(122, 126, 'AstroTerm'),(129, 134, 'AstroTerm'), (175, 183, 'AstroTerm')]}),     \n",
    " ('\\uf63c\\uf6dc\\uf63a focus \\uf6dc\\uf63c\\uf63b, \\uf6dc\\uf63c\\uf63d, \\uf6dc\\uf63c\\uf640 – focusing \\uf6dc\\uf63d\\uf63c, \\uf6dc\\uf63e\\uf63e folded refractor',\n",
    "  {'entities': [(51, 60, 'AstroTerm')]}),\n",
    " (\"A mount's top, or head, can be either alt-azimuth (turning side to side, up and down) or equatorial (turning parallel to the celestial coordinate system).\",\n",
    "  {'entities': [(42, 49, 'AstroTerm')]}),\n",
    " ('A complex radio source at the centre of the Milky Way      Galaxy.',\n",
    "  {'entities': [(44, 53, 'AstroTerm'), (59, 65, 'AstroTerm')]}),\n",
    " ('The longest diameter of an ellipse.',\n",
    "  {'entities': [(27, 34, 'AstroTerm')]}),\n",
    " ('Sidereal time enables the hour angle of an object      to be found from its right ascension (hour angle sidereal      time - right ascension).\\n',\n",
    "  {'entities': [(0, 13, 'AstroTerm')]}),\n",
    " (\"Earth's orbit; those that orbit or spin clockwise have retrograde motion.\",\n",
    "  {'entities': [(8,13, 'AstroTerm'), (35, 39, 'AstroTerm')]}),\n",
    " ('The Newtonian reflector, designed by Isaac Newton, has a small second mirror mounted diagonally near the front of the tube to divert the light sideways and out to your eye.',\n",
    "  {'entities': [(37, 49, 'AstroTerm')]}),\n",
    " ('population l star.', {'entities': [(13, 17, 'AstroTerm')]}),\n",
    " ('The deflection of light from a remote source caused by the presence of an intervening mass.',\n",
    "  {'entities': [(86, 90, 'AstroTerm')]}),\n",
    " ('A natural satellite that orbits a planet.',\n",
    "  {'entities': [(10, 19, 'AstroTerm')]})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 100) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--/Users/abdelhak/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/spacy/language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"A description of gravity formulated by Albert Eins...\" with entities \"[(39, 54, 'AstroTerm'), (76, 84, 'AstroTerm')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "100% (100 of 100) |######################| Elapsed Time: 0:00:34 Time:  0:00:34\n"
     ]
    }
   ],
   "source": [
    "nlp3 = train_spacy(TEST_DATA, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Moon', 'AstroTerm'), ('orbit', 'AstroTerm'), ('eclipses', 'AstroTerm')]\n",
      "Tokens [('If', '', 2), ('the', '', 2), ('Moon', '', 2), ('orbited', '', 2), ('in', '', 2), ('the', '', 2), ('same', '', 2), ('plane', '', 2), ('as', '', 2), ('the', '', 2), ('Earth', '', 2), ('around', '', 2), ('the', '', 2), ('Sun', '', 2), (',', '', 2), ('the', '', 2), ('Sun', '', 2), ('would', '', 2), ('be', '', 2), ('eclipsed', '', 2), ('every', '', 2), ('month', '', 2), (',', '', 2), ('but', '', 2), ('the', '', 2), ('Moon', 'AstroTerm', 3), (\"'s\", '', 2), ('orbit', 'AstroTerm', 3), ('is', '', 2), ('in', '', 2), ('a', '', 2), ('slightly', '', 2), ('inclined', '', 2), ('plane', '', 2), (',', '', 2), ('making', '', 2), ('eclipses', 'AstroTerm', 3), ('of', '', 2), ('the', '', 2), ('Sun', '', 2), ('rare', '', 2), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp3(\"If the Moon orbited in the same plane as the Earth around the Sun, the Sun would be eclipsed every month, but the Moon's orbit is in a slightly inclined plane, making eclipses of the Sun rare.\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
